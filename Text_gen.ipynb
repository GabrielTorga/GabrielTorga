{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/Lqk7tJmBGuA4SzXJbZrc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielTorga/GabrielTorga/blob/main/Text_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F497bKtHvZNg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file(\n",
        "    \"shakespeare.txt\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78TBzfg_vcvJ",
        "outputId": "1710962e-e2a5-4c4b-8c30-fe43536b7169"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "print(f\"Length of text: {len(text)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGClhz-Jvg7B",
        "outputId": "e82494e5-9915-444a-e902-a39a22c71777"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:60])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3vr1Sl0w9Jv",
        "outputId": "0b157723-3cff-4843-fbc3-11a6b4d1a52b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sorting unique characters\n",
        "vocab = sorted(set(text))\n",
        "print(vocab[:10])\n",
        "print(f\"{len(vocab)} unique characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsRybOUcw-j_",
        "outputId": "512ca002-05f3-469f-fda0-1ac7720aaf46"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3']\n",
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = text[:60]\n",
        "\n",
        "# Characters into tokens\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rW1-pj5xDOz",
        "outputId": "6cb83443-1954-4d40-9cbd-2730dcdcb49c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(60,), dtype=string, numpy=\n",
              "array([b'F', b'i', b'r', b's', b't', b' ', b'C', b'i', b't', b'i', b'z',\n",
              "       b'e', b'n', b':', b'\\n', b'B', b'e', b'f', b'o', b'r', b'e', b' ',\n",
              "       b'w', b'e', b' ', b'p', b'r', b'o', b'c', b'e', b'e', b'd', b' ',\n",
              "       b'a', b'n', b'y', b' ', b'f', b'u', b'r', b't', b'h', b'e', b'r',\n",
              "       b',', b' ', b'h', b'e', b'a', b'r', b' ', b'm', b'e', b' ', b's',\n",
              "       b'p', b'e', b'a', b'k', b'.'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens to chars IDs\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None\n",
        ")\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5Qquj3MySm-",
        "outputId": "2acb63a2-dd8e-43fc-9c20-27b9fbd9a0e7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(60,), dtype=int64, numpy=\n",
              "array([19, 48, 57, 58, 59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 15, 44,\n",
              "       45, 54, 57, 44,  2, 62, 44,  2, 55, 57, 54, 42, 44, 44, 43,  2, 40,\n",
              "       53, 64,  2, 45, 60, 57, 59, 47, 44, 57,  7,  2, 47, 44, 40, 57,  2,\n",
              "       52, 44,  2, 58, 55, 44, 40, 50,  9])>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recover human-readable strings\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRPGXqvey-D8",
        "outputId": "d3a1344e-44c9-4df1-ea27-f779bc043b02"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'n'>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciMwMcA7zDNH",
        "outputId": "f3d73672-8dd4-481b-d17a-ee4bd6c44a24"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'First Citizen:\\nBefore we proceed any further, hear me speak.'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "text_from_ids(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkLN8FeAzIyZ",
        "outputId": "c326ba78-de7f-4fb9-ce2b-23b55dc1dc81"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'First Citizen:\\nBefore we proceed any further, hear me speak.'>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create traning dataset**"
      ],
      "metadata": {
        "id": "rQzjF9MfRyEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32Ru36V_5trB",
        "outputId": "c690abc4-ebd7-4748-8993-b7d250d30729"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "A-K8Zjqo5u3P"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(13):\n",
        "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMSeyBDj6AjT",
        "outputId": "32b438e2-e225-4769-9f8a-5f9bd60e0d8b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "z\n",
            "e\n",
            "n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)"
      ],
      "metadata": {
        "id": "y2wBIKHw6Cjt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY72-JQc6Iw6",
        "outputId": "73c23100-9060-498b-bd65-f9078322fefa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib0Hsum36TB_",
        "outputId": "95cbb1ee-2277-4172-ba08-15c1aaf9f5f9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For training you'll need a dataset of (input, label) pairs. Where input and label are sequences.\n",
        "# At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n"
      ],
      "metadata": {
        "id": "bdH1fbF08zED"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(\"Tensorflow\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrfNQGkm9Md6",
        "outputId": "68131686-58e4-4caa-cfdb-b6d0926943b8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input one char and the output is the next one (to predict)\n",
        "\n",
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXMdHuaT9IOL",
        "outputId": "b9fbc4c1-07e5-4edf-d63f-d59a751feb3d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "M6R8bgm-9Nbd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "BuRbhasU9TRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24005526-7087-4cab-aca5-704b204171cb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create training batches**"
      ],
      "metadata": {
        "id": "rD8W1ULSVi9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset.shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtlwG8jhVTNP",
        "outputId": "5ab4283c-68f2-4164-ad23-96e5a3f70f1b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build The Model**\n",
        "\n",
        "\n",
        "- tf.keras.layers.Embedding: *The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;*\n",
        "- tf.keras.layers.GRU: *A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)*\n",
        "- tf.keras.layers.Dense: *The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.*\n"
      ],
      "metadata": {
        "id": "LCibZjwoblEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "F8x21a4AVxA6"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        # TODO - Create an embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # TODO - Create a GRU layer\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units, return_sequences=True, return_state=True\n",
        "        )\n",
        "        # TODO - Finally connect it with a dense layer\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        # since we are training a text generation model,\n",
        "        # we use the previous state, in training. If there is no state,\n",
        "        # then we initialize the state\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x\n"
      ],
      "metadata": {
        "id": "85hxGBEVc8cD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ],
      "metadata": {
        "id": "lky68rDNc9Cs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character."
      ],
      "metadata": {
        "id": "_yIZAgdLdB4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "        \"# (batch_size, sequence_length, vocab_size)\",\n",
        "    )\n",
        "# Checking the shape of the output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO0NpW_zc-t6",
        "outputId": "ac51ed3c-620c-45f4-ab6c-857e068c0b36"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKtqZmF-dAvf",
        "outputId": "497038af-3c5b-4d14-af14-118040eb4ecc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying for a simple batch\n",
        "sampled_indices = tf.random.categorical(\n",
        "    example_batch_predictions[0], num_samples=1\n",
        ")\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "UZen4V_6dT_6"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This gives us, at each timestep, a prediction of the next character index:\n",
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvtotSAcdsLB",
        "outputId": "25fb3ed4-fad1-40a0-face-80f6fea0ac57"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([26, 51, 41, 64,  5, 18, 47, 16, 53, 62,  3, 65, 22, 54, 24, 21, 38,\n",
              "       43, 18, 14, 29, 60, 58, 23,  0, 53, 17, 11, 19, 51, 15, 37, 43, 63,\n",
              "       44, 27,  9, 11, 44, 32, 16, 56, 30, 16, 35, 23, 56, 57,  8, 16, 51,\n",
              "       56, 20, 24, 41,  4, 46, 19,  1, 38, 62, 57, 48, 65, 34, 28, 57,  9,\n",
              "       60, 26, 19, 43, 20, 54, 64,  3, 50, 20, 49, 57,  1, 22, 43, 37, 63,\n",
              "       42, 48, 27, 65, 21, 58, 23, 19, 24, 31, 63,  5,  0, 15, 34])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j2klAoBeF21",
        "outputId": "5c841a0b-ea5b-491a-f9ec-f0bcc8bb5d80"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\" the world's deceit\\nNor more can you distinguish of a man\\nThan of his outward show; which, God he kn\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b'Mlby&EhCnw!zIoKHYdEAPusJ[UNK]nD:FlBXdxeN.:eSCqQCVJqr-ClqGKb$gF\\nYwrizUOr.uMFdGoy!kGjr\\nIdXxciNzHsJFKRx&[UNK]BU'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8PF4jp4veOxA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}